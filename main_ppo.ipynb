{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! export WANDB_NOTEBOOK_NAME=\"main.ipynb\"\n",
    "import wandb\n",
    "\n",
    "PROJECT_NAME = \"Reinforcement Learning (DQN) - JVM-GC\"\n",
    "WANDB_KEY = \"4b077df3688052b0f43705d6b4d712c05fb979b7\"\n",
    "config = dict(\n",
    "    competition = PROJECT_NAME,\n",
    "    _wandb_kernel = 'lemon',\n",
    "    seed = 42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(filename=\"gc-ml-ppo.log\",\n",
    "                    filemode='a',\n",
    "                    format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s')\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "# logger.setLevel(logging.DEBUG)\n",
    "logger.setLevel(logging.INFO)\n",
    "# logger.error(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tf_agents.agents import PPOAgent\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.specs import tensor_spec, from_spec\n",
    "from tf_agents import trajectories\n",
    "from tf_agents.networks.actor_distribution_network import ActorDistributionNetwork\n",
    "from tf_agents.networks.value_network import ValueNetwork\n",
    "from tf_agents.drivers.dynamic_episode_driver import DynamicEpisodeDriver\n",
    "\n",
    "# from env.PyEnvironments import CurveEnv, CurveMultipleEnv, JVMEnv\n",
    "# from env.PyEnvironments import JVMEnv\n",
    "from env.PyEnvironmentsTest import JVMEnv # !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initialized a JVM Environment!\n",
      " JDK: jdk-11.0.20.1.jdk/bin,\n",
      " Benchmark: avrora (dacapo-bench.jar),\n",
      " Number of iterations: 5,\n",
      " Goal: avgGCPause,\n",
      " Number of JVM options: 2,\n",
      " JVM options: {'MaxTenuringThreshold': {'min': 1, 'max': 16}, 'ParallelGCThreads': {'min': 4, 'max': 24}},\n",
      " Env. default state: [list([7, 12]) 0.47],\n",
      " Env. default goal value: 0.47,\n",
      "\n",
      "Successfully initialized a JVM Environment!\n",
      " JDK: jdk-11.0.20.1.jdk/bin,\n",
      " Benchmark: kafka (dacapo-bench.jar),\n",
      " Number of iterations: 5,\n",
      " Goal: avgGCPause,\n",
      " Number of JVM options: 2,\n",
      " JVM options: {'MaxTenuringThreshold': {'min': 1, 'max': 16}, 'ParallelGCThreads': {'min': 4, 'max': 24}},\n",
      " Env. default state: [list([7, 12]) 0.34],\n",
      " Env. default goal value: 0.34,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST_JDK_PATH = \"/Users/ellkrauze/projects/gc-ml/jdk-11.0.20.1.jdk\"\n",
    "# BM = \"avrora\"\n",
    "# BM_TEST = \"kafka\"\n",
    "# BM_PATH = \"/Users/ellkrauze/projects/gc-ml/dacapo-bench.jar\"\n",
    "# CALLBACK_PATH = \"/home/vsakovskaya/gc-ml/dacapo/DacapoCallback/DacapoChopin/VMStatCallback.java\"\n",
    "dataset_path = \"dataset/data\"\n",
    "tempdir = \"tmp\"\n",
    "checkpoint_dir = os.path.join(tempdir, 'checkpoint')\n",
    "policy_dir = os.path.join(tempdir, 'policy')\n",
    "\n",
    "env_args = {\n",
    "    \"jdk_path\": \"jdk-11.0.20.1.jdk\",\n",
    "    \"bm_path\": \"dacapo-bench.jar\",\n",
    "    \"gc_viewer_jar\": \"gcviewer-1.36.jar\",\n",
    "    \"callback_path\": \"callback/VMStatCallback.java\",\n",
    "    \"n\": 5,\n",
    "    \"goal\": \"avgGCPause\",\n",
    "    \"verbose\": False,\n",
    "}\n",
    "\n",
    "env = JVMEnv(bm_name=\"avrora\", **env_args)\n",
    "\n",
    "env_test = JVMEnv(bm_name=\"kafka\", **env_args)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(env, isolation=True)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(env, isolation=True)\n",
    "test_env = tf_py_environment.TFPyEnvironment(env_test)\n",
    "\n",
    "action_spec = from_spec(train_env.action_spec())\n",
    "observation_spec = from_spec(train_env.observation_spec())\n",
    "reward_spec = from_spec(train_env.reward_spec())\n",
    "time_step_spec = trajectories.time_step_spec(observation_spec, reward_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_net(action_spec, fc_layer_params = (100, 75, 50)):\n",
    "    action_tensor_spec = tensor_spec.from_spec(action_spec)\n",
    "    num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "    def dense_layer(num_units):\n",
    "        \"\"\"\n",
    "        Define a helper function to create Dense layers configured \n",
    "        with the right activation and kernel initializer.\n",
    "        \"\"\"\n",
    "        return tf.keras.layers.Dense(\n",
    "            num_units,\n",
    "            activation=tf.keras.activations.relu,\n",
    "            kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "                scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "    # QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
    "    # with `num_actions` units to generate one q_value per available action as\n",
    "    # its output.\n",
    "    dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "    q_values_layer = tf.keras.layers.Dense(\n",
    "        num_actions,\n",
    "        activation=None,\n",
    "        kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "            minval=-0.03, maxval=0.03),\n",
    "        bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "    q_net = sequential.Sequential(dense_layers + [q_values_layer])\n",
    "    return q_net\n",
    "\n",
    "def collect_step(environment, policy, replay_buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    # reward = next_time_step.reward\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "    return traj\n",
    "\n",
    "def save_dataset(data, path):\n",
    "    step_counter = tf.Variable(0, trainable=False)\n",
    "    checkpoint_prefix = \"checkpoint\"\n",
    "    checkpoint_args = {\n",
    "        \"checkpoint_interval\": 50,\n",
    "        \"step_counter\": step_counter,\n",
    "        \"directory\": checkpoint_prefix,\n",
    "        \"max_to_keep\": 3,\n",
    "    }\n",
    "    try:\n",
    "        data.save(path, checkpoint_args=checkpoint_args)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def save_rb(replay_buffer, path):\n",
    "    tf.train.Checkpoint(rb = replay_buffer).save(path)\n",
    "\n",
    "def restore_rb(replay_buffer, path):\n",
    "    tf.train.Checkpoint(rb = replay_buffer).restore(path)\n",
    "\n",
    "def get_dataset(replay_buffer, size, batch_size, collect_data_spec, n_step_update, create: bool = True, save: bool = False):\n",
    "    \n",
    "    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "        data_spec=collect_data_spec, # agent.collect_data_spec\n",
    "        batch_size=1, # train_env.batch_size\n",
    "        max_length=size) # capacity\n",
    "\n",
    "    if create:\n",
    "        for _ in tqdm(range(size)):\n",
    "            traj = collect_step(train_env, random_policy, replay_buffer)\n",
    "            replay_buffer.add_batch(traj)\n",
    "\n",
    "        if save: save_rb(replay_buffer, dataset_path)\n",
    "    else: \n",
    "        restore_rb(replay_buffer, dataset_path+'-1')\n",
    "\n",
    "    dataset = replay_buffer.as_dataset(\n",
    "        num_parallel_calls=3, \n",
    "        sample_batch_size=batch_size,\n",
    "        num_steps=n_step_update + 1, \n",
    "        single_deterministic_pass=False).prefetch(3)\n",
    "    # iterator = iter(dataset)\n",
    "    return dataset\n",
    "    \n",
    "def compute_avg_return(environment, policy, num_episodes=50):\n",
    "    \"\"\"\n",
    "    Computes the average return of a policy, \n",
    "    given the policy, environment, and a number of episodes.\n",
    "\n",
    "    Note: for non-episodic tasks.\n",
    "    \"\"\"\n",
    "    total_return = 0.0\n",
    "    time_step = environment.reset()\n",
    "    for _ in tqdm(range(num_episodes)):\n",
    "        action_step = policy.action(time_step, seed=42)\n",
    "        time_step = environment.step(action_step.action)\n",
    "        total_return += time_step.reward\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]\n",
    "\n",
    "def compute_avg_return_episodic(environment, policy, num_episodes=10, patience=100):\n",
    "    \"\"\"\n",
    "    Computes the average return of a policy, \n",
    "    given the policy, environment, and a number of episodes.\n",
    "\n",
    "    Note: for episodic tasks.\n",
    "    \"\"\"\n",
    "    total_return = 0.0\n",
    "    for _ in tqdm(range(num_episodes)):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "        i = 0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            if i >= patience:\n",
    "                break\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            # print(\"time step:\", action_step)\n",
    "            # print(\"action:\", action_step.action)\n",
    "            obs = time_step.observation.numpy()[0]\n",
    "            rwd = time_step.reward.numpy()[0]\n",
    "            # logger.debug(f\"[COMPUTE AVERAGE RETURN EPISODIC] action: {action_step.action}, obs: {obs}, reward: {rwd}\")\n",
    "            episode_return += time_step.reward\n",
    "            i += 1\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]\n",
    "\n",
    "def create_networks(observation_spec, action_spec, fc_layer_params):\n",
    "    actor_net = ActorDistributionNetwork(\n",
    "            observation_spec,\n",
    "            action_spec,\n",
    "            fc_layer_params=fc_layer_params,\n",
    "            activation_fn=tf.keras.activations.tanh)\n",
    "    \n",
    "    value_net = ValueNetwork(\n",
    "            observation_spec,\n",
    "            fc_layer_params=fc_layer_params,\n",
    "            activation_fn=tf.keras.activations.tanh)\n",
    "\n",
    "    return actor_net, value_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_steps_per_iteration = 1\n",
    "dataset_size = 1000\n",
    "# fc_layer_params = (100, 75, 50)\n",
    "# fc_layer_params = (200, 100)\n",
    "fc_layer_params = (128, 128, 128)\n",
    "\n",
    "batch_size = 24\n",
    "# learning_rate = 1e-3\n",
    "learning_rate = 0.0005\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 100  # @param {type:\"integer\"}\n",
    "\n",
    "train_episodes_per_iteration = 10\n",
    "n_step_update = 4\n",
    "actor_net, value_net = create_networks(observation_spec, action_spec, fc_layer_params)\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "agent_args = {\n",
    "    \"optimizer\": tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate),\n",
    "    \"actor_net\": actor_net,\n",
    "    \"value_net\": value_net,\n",
    "    \"train_step_counter\": global_step,\n",
    "    \"importance_ratio_clipping\": 0.1,\n",
    "    \"num_epochs\": 20,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPOAgent(\n",
    "  time_step_spec,\n",
    "  action_spec,\n",
    "  **agent_args\n",
    ")\n",
    "\n",
    "agent.initialize()\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(\n",
    "  time_step_spec = time_step_spec,\n",
    "  action_spec = action_spec,\n",
    "  automatic_state_reset=True,\n",
    "  clip=False,\n",
    "  emit_log_probability=True\n",
    ")\n",
    "\n",
    "def get_rb_and_cd(_env, _agent):\n",
    "  replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "      data_spec=_agent.collect_data_spec, # agent.collect_data_spec\n",
    "      batch_size=_env.batch_size, # train_env.batch_size\n",
    "      max_length=dataset_size) # capacity\n",
    "\n",
    "  replay_buffer_observer = replay_buffer.add_batch\n",
    "\n",
    "  collect_driver = DynamicEpisodeDriver(\n",
    "      _env,\n",
    "      _agent.collect_policy,\n",
    "      observers=[replay_buffer_observer], # + train_metrics,\n",
    "      num_episodes=train_episodes_per_iteration)\n",
    "  \n",
    "  return replay_buffer, collect_driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.plots_util import plot_dataset, plot_goal_heatmap\n",
    "\n",
    "# plot_dataset(replay_buffer)\n",
    "# plot_goal_heatmap(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(_agent, _env_train, _env_val, \n",
    "          steps: int = 5000, \n",
    "          use_wandb: bool = False,\n",
    "          eval_interval: int=100):\n",
    "    \"\"\"\n",
    "    Train reinforcement learning agent and evaluate\n",
    "    performance on a separate environment.\n",
    "    \"\"\"\n",
    "    seed = 42\n",
    "    total_return = 0.0\n",
    "\n",
    "    _env_train.reset()\n",
    "    _env_val.reset()\n",
    "\n",
    "    replay_buffer, collect_driver = get_rb_and_cd(_env_train, _agent)\n",
    "    \n",
    "    _agent.train = common.function(_agent.train)\n",
    "    collect_driver.run = common.function(collect_driver.run)\n",
    "    replay_buffer.gather_all = common.function(replay_buffer.gather_all)\n",
    "    _agent.train_step_counter.assign(0)\n",
    "    _agent.initialize()\n",
    "    time_step = _env_val.reset()\n",
    "    policy_state = _agent.collect_policy.get_initial_state(_env_train.batch_size)\n",
    "\n",
    "    loss = []\n",
    "    observations = []\n",
    "\n",
    "    rewards = []\n",
    "    # avg_reward = compute_avg_return_episodic(_env_val, _agent.policy, num_episodes=20)\n",
    "    # rewards = [avg_reward]\n",
    "\n",
    "    for step in tqdm(range(steps)):\n",
    "\n",
    "        collect_driver.run()\n",
    "\n",
    "        experience = replay_buffer.gather_all()\n",
    "        train_loss = _agent.train(experience)\n",
    "        replay_buffer.clear()\n",
    "\n",
    "        # Calculate a reward on evaluation environment\n",
    "        # policy_step = _agent.policy.action(time_step, seed=seed)\n",
    "        # time_step = _env_val.step(policy_step.action)\n",
    "        # rwd = time_step.reward.numpy()[0]\n",
    "        # obs = time_step.observation.numpy()[0]\n",
    "        # print()\n",
    "\n",
    "        # logger.debug(f\"action: {policy_step.action}, obs: {obs}, reward: {rwd}\")\n",
    "        # total_return += rwd # Calculate a sum of rewards\n",
    "        # print(obs)\n",
    "        # replay_buffer.clear()\n",
    "        # step = _agent.train_step_counter.numpy()\n",
    "\n",
    "        if step % eval_interval == 0:\n",
    "            # avg_reward = total_return / eval_interval\n",
    "            # avg_reward = compute_avg_return(_env_val, _agent.policy, num_episodes=50)\n",
    "            avg_reward = compute_avg_return_episodic(_env_val, _agent.policy, num_episodes=20)\n",
    "            total_return = 0.0 # reset\n",
    "\n",
    "            loss.append(train_loss.loss.numpy())\n",
    "            # observations.append(obs)\n",
    "            rewards.append(avg_reward)\n",
    "\n",
    "            # wandb logger for tuning hyperparameters\n",
    "            if use_wandb:\n",
    "                wandb.log({'loss': train_loss.loss, 'reward': avg_reward})\n",
    "\n",
    "            print(f\"step = {step}: loss = {train_loss.loss}, reward = {avg_reward}\")\n",
    "    \n",
    "    return loss, observations, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env.reset()\n",
    "replay_buffer, collect_driver = get_rb_and_cd(train_env, agent)\n",
    "agent.train = common.function(agent.train)\n",
    "collect_driver.run = common.function(collect_driver.run)\n",
    "replay_buffer.gather_all = common.function(replay_buffer.gather_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_driver.run()\n",
    "\n",
    "experience = replay_buffer.gather_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = agent.train(experience)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "replay_buffer.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:10<00:00,  1.88it/s]\n",
      "  0%|          | 1/3000 [00:15<12:40:49, 15.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0: loss = 1.2406761646270752, reward = 531.3624267578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 28/3000 [01:48<7:46:09,  9.41s/it]"
     ]
    }
   ],
   "source": [
    "num_steps = 3000\n",
    "# num_steps = 1000\n",
    "loss, observations, rewards = train(agent, train_env, eval_env, steps = num_steps, eval_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (30,) and (31,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ellkrauze/projects/gc-ml/main_ppo.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ellkrauze/projects/gc-ml/main_ppo.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m color \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtab:blue\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ellkrauze/projects/gc-ml/main_ppo.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m ax2\u001b[39m.\u001b[39mset_ylabel(\u001b[39m'\u001b[39m\u001b[39mrewards\u001b[39m\u001b[39m'\u001b[39m, color\u001b[39m=\u001b[39mcolor)  \u001b[39m# we already handled the x-label with ax1\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ellkrauze/projects/gc-ml/main_ppo.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m ax2\u001b[39m.\u001b[39;49mplot(steps, rewards[:\u001b[39m200\u001b[39;49m], color\u001b[39m=\u001b[39;49mcolor)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ellkrauze/projects/gc-ml/main_ppo.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m ax2\u001b[39m.\u001b[39mtick_params(axis\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m, labelcolor\u001b[39m=\u001b[39mcolor)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ellkrauze/projects/gc-ml/main_ppo.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m fig\u001b[39m.\u001b[39mtight_layout()  \u001b[39m# otherwise the right y-label is slightly clipped\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/gc-ml/gc-ml-env/lib/python3.8/site-packages/matplotlib/axes/_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[0;32m-> 1688\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[1;32m   1689\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[1;32m   1690\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/projects/gc-ml/gc-ml-env/lib/python3.8/site-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(\n\u001b[1;32m    312\u001b[0m     this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39;49mambiguous_fmt_datakey)\n",
      "File \u001b[0;32m~/projects/gc-ml/gc-ml-env/lib/python3.8/site-packages/matplotlib/axes/_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39mupdate_units(y)\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[0;32m--> 504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have same first dimension, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhave shapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    506\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    507\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y can be no greater than 2D, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (30,) and (31,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "steps = [step for step in range(0, num_steps, 100)]\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('steps')\n",
    "ax1.set_ylabel('loss', color=color)\n",
    "ax1.plot(steps, loss, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('rewards', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(steps, rewards[:200], color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:16<00:00,  3.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.6166726"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_avg_return_episodic(test_env, agent.policy, num_episodes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/ellkrauze/projects/gc-ml/main_ppo.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ellkrauze/projects/gc-ml/main_ppo.ipynb#Y115sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m loss_test, observations_test, rewards_test \u001b[39m=\u001b[39m train(agent, test_env, test_env, steps \u001b[39m=\u001b[39m num_steps, eval_interval\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "loss_test, observations_test, rewards_test = train(agent, test_env, test_env, steps = num_steps, eval_interval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_checkpointer.save(global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: srh8mikx\n",
      "Sweep URL: https://wandb.ai/cold-machines/Reinforcement%20Learning%20%28DQN%29%20-%20JVM-GC/sweeps/srh8mikx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 86wuplng with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepsilon_greedy: 0.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.009081865474555018\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttarget_update_period: 100\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvaleriia-sakovskaia\u001b[0m (\u001b[33mcold-machines\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vsakovskaya/gc-ml/wandb/run-20231031_074257-86wuplng</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cold-machines/Reinforcement%20Learning%20%28DQN%29%20-%20JVM-GC/runs/86wuplng' target=\"_blank\">rare-sweep-1</a></strong> to <a href='https://wandb.ai/cold-machines/Reinforcement%20Learning%20%28DQN%29%20-%20JVM-GC' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cold-machines/Reinforcement%20Learning%20%28DQN%29%20-%20JVM-GC/sweeps/srh8mikx' target=\"_blank\">https://wandb.ai/cold-machines/Reinforcement%20Learning%20%28DQN%29%20-%20JVM-GC/sweeps/srh8mikx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cold-machines/Reinforcement%20Learning%20%28DQN%29%20-%20JVM-GC' target=\"_blank\">https://wandb.ai/cold-machines/Reinforcement%20Learning%20%28DQN%29%20-%20JVM-GC</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cold-machines/Reinforcement%20Learning%20%28DQN%29%20-%20JVM-GC/sweeps/srh8mikx' target=\"_blank\">https://wandb.ai/cold-machines/Reinforcement%20Learning%20%28DQN%29%20-%20JVM-GC/sweeps/srh8mikx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cold-machines/Reinforcement%20Learning%20%28DQN%29%20-%20JVM-GC/runs/86wuplng' target=\"_blank\">https://wandb.ai/cold-machines/Reinforcement%20Learning%20%28DQN%29%20-%20JVM-GC/runs/86wuplng</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 149.90it/s]\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vsakovskaya/my-env/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vsakovskaya/my-env/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
      "100%|██████████| 50/50 [00:00<00:00, 138.65it/s]] \n",
      "  2%|▏         | 101/5000 [00:16<18:44,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 100: loss = 0.051988594233989716, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 142.71it/s]s]\n",
      "  4%|▍         | 201/5000 [00:32<17:56,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 200: loss = 0.04959993064403534, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 150.07it/s]s]\n",
      "  6%|▌         | 301/5000 [00:48<17:17,  4.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 300: loss = 0.05310416966676712, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 149.06it/s]s]\n",
      "  8%|▊         | 401/5000 [01:04<21:05,  3.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 400: loss = 0.07190939784049988, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 149.21it/s]s]\n",
      " 10%|█         | 501/5000 [01:20<16:40,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 500: loss = 0.06140190362930298, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 145.06it/s]s]\n",
      " 12%|█▏        | 601/5000 [01:36<16:26,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 600: loss = 0.07814410328865051, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 142.03it/s]s]\n",
      " 14%|█▍        | 701/5000 [01:53<16:30,  4.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 700: loss = 0.05395403504371643, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 148.61it/s]s]\n",
      " 16%|█▌        | 801/5000 [02:08<15:39,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 800: loss = 0.05778862535953522, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 149.60it/s]s]\n",
      " 18%|█▊        | 901/5000 [02:25<14:59,  4.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 900: loss = 0.05692460760474205, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 148.17it/s]s]\n",
      " 20%|██        | 1001/5000 [02:41<15:17,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 1000: loss = 0.059485163539648056, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 144.85it/s]/s]\n",
      " 22%|██▏       | 1101/5000 [02:57<14:36,  4.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 1100: loss = 0.07164233177900314, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 139.96it/s]/s]\n",
      " 24%|██▍       | 1201/5000 [03:14<14:20,  4.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 1200: loss = 0.04963871091604233, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 151.36it/s]/s]\n",
      " 26%|██▌       | 1301/5000 [03:30<14:23,  4.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 1300: loss = 0.05595877766609192, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 146.66it/s]/s]\n",
      " 28%|██▊       | 1401/5000 [03:46<13:25,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 1400: loss = 0.05879542976617813, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 149.67it/s]/s]\n",
      " 30%|███       | 1501/5000 [04:02<12:52,  4.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 1500: loss = 0.06716286391019821, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 131.84it/s]/s]\n",
      " 32%|███▏      | 1601/5000 [04:18<13:14,  4.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 1600: loss = 0.05800986662507057, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 145.37it/s]/s]\n",
      " 34%|███▍      | 1701/5000 [04:35<12:29,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 1700: loss = 0.06368903815746307, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 139.72it/s]/s]\n",
      " 36%|███▌      | 1801/5000 [04:51<12:38,  4.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 1800: loss = 0.05840213596820831, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 144.50it/s]/s]\n",
      " 38%|███▊      | 1901/5000 [05:07<11:47,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 1900: loss = 0.06790091097354889, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 148.98it/s]/s]\n",
      " 40%|████      | 2001/5000 [05:24<11:03,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 2000: loss = 0.05456307530403137, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 147.69it/s]/s]\n",
      " 42%|████▏     | 2101/5000 [05:39<10:43,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 2100: loss = 0.05199477821588516, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 146.01it/s]/s]\n",
      " 44%|████▍     | 2201/5000 [05:56<10:51,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 2200: loss = 0.046316057443618774, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 151.46it/s]/s]\n",
      " 46%|████▌     | 2301/5000 [06:12<09:51,  4.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 2300: loss = 0.0451720654964447, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 139.11it/s]/s]\n",
      " 48%|████▊     | 2401/5000 [06:28<09:59,  4.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 2400: loss = 0.05063679814338684, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 149.10it/s]/s]\n",
      " 50%|█████     | 2501/5000 [06:44<09:13,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 2500: loss = 0.05682305246591568, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 151.48it/s]/s]\n",
      " 52%|█████▏    | 2601/5000 [07:01<08:57,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 2600: loss = 0.057316817343235016, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 148.48it/s]/s]\n",
      " 54%|█████▍    | 2701/5000 [07:17<08:40,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 2700: loss = 0.06233186647295952, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 147.44it/s]/s]\n",
      " 56%|█████▌    | 2801/5000 [07:34<08:08,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 2800: loss = 0.030263567343354225, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 150.17it/s]/s]\n",
      " 58%|█████▊    | 2901/5000 [07:49<07:45,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 2900: loss = 0.12275780737400055, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 145.94it/s]/s]\n",
      " 60%|██████    | 3001/5000 [08:06<07:35,  4.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3000: loss = 0.0640171468257904, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 145.29it/s]/s]\n",
      " 62%|██████▏   | 3101/5000 [08:22<07:06,  4.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3100: loss = 0.06743437051773071, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 142.10it/s]/s]\n",
      " 64%|██████▍   | 3201/5000 [08:39<07:00,  4.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3200: loss = 0.07279805094003677, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 145.21it/s]/s]\n",
      " 66%|██████▌   | 3301/5000 [08:55<06:22,  4.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3300: loss = 0.03942070156335831, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 148.63it/s]/s]\n",
      " 68%|██████▊   | 3401/5000 [09:12<06:25,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3400: loss = 0.06413926929235458, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 138.84it/s]/s]\n",
      " 70%|███████   | 3501/5000 [09:27<05:42,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3500: loss = 0.04303019493818283, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 151.39it/s]/s]\n",
      " 72%|███████▏  | 3601/5000 [09:43<05:13,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3600: loss = 0.0326034352183342, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 145.81it/s]/s]\n",
      " 74%|███████▍  | 3701/5000 [10:00<04:56,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3700: loss = 0.06070145219564438, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 147.19it/s]/s]\n",
      " 76%|███████▌  | 3801/5000 [10:16<04:34,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3800: loss = 0.05568353831768036, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 145.26it/s]/s]\n",
      " 78%|███████▊  | 3901/5000 [10:33<04:09,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3900: loss = 0.0671439915895462, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 146.18it/s]/s]\n",
      " 80%|████████  | 4001/5000 [10:49<03:42,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4000: loss = 0.07973361760377884, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 147.49it/s]/s]\n",
      " 82%|████████▏ | 4101/5000 [11:06<03:38,  4.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4100: loss = 0.05744113028049469, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 145.54it/s]/s]\n",
      " 84%|████████▍ | 4201/5000 [11:22<03:00,  4.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4200: loss = 0.04437218979001045, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 149.76it/s]/s]\n",
      " 86%|████████▌ | 4301/5000 [11:37<02:34,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4300: loss = 0.06655722111463547, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 149.96it/s]/s]\n",
      " 88%|████████▊ | 4401/5000 [11:55<02:13,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4400: loss = 0.054657574743032455, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 150.15it/s]/s]\n",
      " 90%|█████████ | 4501/5000 [12:10<01:51,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4500: loss = 0.05559392273426056, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 151.40it/s]/s]\n",
      " 92%|█████████▏| 4601/5000 [12:27<01:32,  4.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4600: loss = 0.062225304543972015, reward = -0.05968308076262474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 4678/5000 [12:39<00:49,  6.51it/s]"
     ]
    }
   ],
   "source": [
    "# Set this to True when you want to run hyperparameter tuning\n",
    "hyperparameter_tuning = True\n",
    "\n",
    "def main():\n",
    "    run = wandb.init(config = config)\n",
    "    \n",
    "    agent_args = {\n",
    "        \"q_network\":            get_q_net(action_spec, fc_layer_params),\n",
    "        \"optimizer\":            tf.compat.v1.train.AdamOptimizer(\n",
    "                                    learning_rate=run.config.learning_rate),\n",
    "        \"n_step_update\":        n_step_update, \n",
    "        \"td_errors_loss_fn\":    tf.keras.losses.MeanSquaredError(),\n",
    "        \"epsilon_greedy\":       run.config.epsilon_greedy,\n",
    "        \"target_update_period\": run.config.target_update_period,\n",
    "        \"gamma\":                run.config.gamma,\n",
    "        \"gradient_clipping\":    1,\n",
    "        \"train_step_counter\":   tf.compat.v1.train.get_or_create_global_step(),\n",
    "    }\n",
    "        \n",
    "    _agent = dqn_agent.DqnAgent(\n",
    "        time_step_spec,\n",
    "        action_spec,\n",
    "        **agent_args\n",
    "    )\n",
    "\n",
    "    loss, _, rewards = train(_agent, train_env, eval_env, steps = 5000, use_wandb = True)\n",
    "    \n",
    "    wandb.log({'avg_loss': np.mean(loss), 'avg_reward': np.mean(rewards)})\n",
    "\n",
    "    del _agent\n",
    "    return\n",
    "\n",
    "# Define sweep config\n",
    "sweep_configuration = {\n",
    "    'method': 'random',\n",
    "    'name': 'rl_greed',\n",
    "    \"metric\": {\n",
    "        \"name\" : \"avg_reward\",\n",
    "        \"goal\": \"maximize\"\n",
    "    },\n",
    "    \"parameters\" : {\n",
    "        \"learning_rate\": {\n",
    "            \"min\": 0.0001,\n",
    "            \"max\": 0.01\n",
    "        },\n",
    "        \"epsilon_greedy\": {'values': [0.1, 0.3, 0.5, 0.7, 0.9]},\n",
    "        \"target_update_period\": {'values': [1, 10, 100, 1000, 10000]},\n",
    "        \"gamma\": {'values': [0.1, 0.3, 0.5, 0.7, 0.9]},\n",
    "    },\n",
    "    'run_cap' : 32\n",
    "}\n",
    "\n",
    "if hyperparameter_tuning:\n",
    "    ! export WANDB_NOTEBOOK_NAME=\"main.ipynb\"\n",
    "    # Initialize sweep by passing in config. (Optional) Provide a name of the project.\n",
    "    sweep_id = wandb.sweep(sweep=sweep_configuration, project=PROJECT_NAME,)\n",
    "\n",
    "    # Start sweep job.\n",
    "    wandb.agent(sweep_id, function=main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(policy, test_env, num_episodes: int=10000):\n",
    "    rewards = []\n",
    "    log_interval = 100\n",
    "    time_step = test_env.reset()\n",
    "    total_return = 0.0\n",
    "    for i in tqdm(range(10000)):\n",
    "        policy_state = policy.action(time_step)\n",
    "        time_step = test_env.step(policy_state.action)\n",
    "        reward = time_step.reward.numpy()[0]\n",
    "        total_return += reward # Calculate a sum of rewards\n",
    "\n",
    "        if i % log_interval == 0:\n",
    "            # avg_return = compute_avg_return(eval_env, agent.policy, 10)\n",
    "            avg_return = total_return / eval_interval\n",
    "            print('step = {0}: Average reward = {1:.5f}'.format(step, avg_return))\n",
    "            rewards.append(avg_return)\n",
    "            total_return = 0.0 # reset\n",
    "    return rewards\n",
    "\n",
    "# test(agent.policy, test_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Episodic Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import episodic_memory\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def embedding_similarity(x1, x2):\n",
    "    assert x1.shape[0] == x2.shape[0]\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    # Inner product between the embeddings in x1\n",
    "    # and the embeddings in x2.\n",
    "    s = np.sum(x1 * x2, axis=-1)\n",
    "\n",
    "    s = s / (np.linalg.norm(x1, axis=-1) * np.linalg.norm(x2, axis=-1) + epsilon)\n",
    "    return 0.5 * (s + 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = replay_buffer.gather_all()[1].numpy().squeeze()\n",
    "actions = replay_buffer.gather_all()[2].numpy().squeeze()\n",
    "size = 15\n",
    "\n",
    "memory_buffer = np.array([[*observations[i], actions[i]] for i in range(size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 3)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_buffer.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = GaussianMixture(n_components=memory_buffer.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianMixture(n_components=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianMixture</label><div class=\"sk-toggleable__content\"><pre>GaussianMixture(n_components=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianMixture(n_components=3)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.fit(memory_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 9.99999917e-01, 8.31505676e-08]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.predict_proba([[7, 12, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df size: 36\n",
      "[[ 4 24  3]\n",
      " [ 4 24  1]\n",
      " [ 7 24  2]\n",
      " [ 7 20  2]\n",
      " [ 7 16  3]\n",
      " [ 7 20  0]\n",
      " [ 4 20  3]\n",
      " [ 4 24  2]\n",
      " [ 4 20  2]\n",
      " [ 4 16  2]\n",
      " [ 4 12  1]\n",
      " [ 7 12  0]\n",
      " [ 4 12  1]\n",
      " [ 7 12  3]\n",
      " [ 7 16  2]]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f\"avrora_synthetic_saved_states.csv\")\n",
    "print(\"df size:\", len(df))\n",
    "states  = {}\n",
    "size = 15 \n",
    "# replay_buffer.capacity\n",
    "for i in range(size):\n",
    "    states [i] = {\"args\": [df[\"MaxTenuringThreshold\"].values[i], df[\"ParallelGCThreads\"].values[i]], \"goal\": df[\"Average GC Pause\"].values[i]}\n",
    "\n",
    "# memory_buffer = np.array([states[i][\"args\"] for i in states.keys()])\n",
    "memory_buffer = np.array([[*observations[i], actions[i]] for i in range(size)])\n",
    "print(memory_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'args': [7, 12], 'goal': 0.47, 'count': 2}}\n"
     ]
    }
   ],
   "source": [
    "perf_states = {}\n",
    "perf_states[0] = {\"args\": [7, 12], \"goal\": 0.47, \"count\": 1} \n",
    "saved_states = [perf_states[i][\"args\"] for i in perf_states.keys()]\n",
    "flags = [7, 12]\n",
    "if flags in saved_states:\n",
    "    for i in perf_states.keys():\n",
    "        \"\"\" \n",
    "        If current state is stored in a cache,\n",
    "        update the state goal value.\n",
    "        \"\"\"\n",
    "        if flags == perf_states[i][\"args\"]:\n",
    "            goal = perf_states[i][\"goal\"]\n",
    "            perf_states[i][\"count\"] += 1\n",
    "\n",
    "print(perf_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in perf_states.keys():\n",
    "    if [7, 12] == perf_states[i][\"args\"]:\n",
    "        print(perf_states[i][\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9975155087566253"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1 + 0.01)**(-1/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96638056, 0.96084794, 0.98113075, 0.98760397, 0.99632883,\n",
       "       0.97980966, 0.97269349, 0.9640097 , 0.97034901, 0.97857303,\n",
       "       0.98519848, 0.98873447, 0.98519848, 1.        , 0.99432438])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation = np.array([[7, 12, 3]] * size)\n",
    "similarity = embedding_similarity(observation, memory_buffer[:size])\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trajectory(\n",
       "{'action': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n",
       " 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.5], dtype=float32)>,\n",
       " 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n",
       " 'observation': <tf.Tensor: shape=(1, 2), dtype=int64, numpy=array([[ 7, 12]])>,\n",
       " 'policy_info': (),\n",
       " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.], dtype=float32)>,\n",
       " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-11.512925464970229"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "math.log(0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/dn/p1_3j9fs78g4wn8r0hcrqks40000gn/T/ipykernel_2341/2210688016.py:1: ReplayBuffer.gather_all (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=True)` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/dn/p1_3j9fs78g4wn8r0hcrqks40000gn/T/ipykernel_2341/2210688016.py:1: ReplayBuffer.gather_all (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=True)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "[0] [ 7 12]\n",
      "[10] [ 7 16]\n",
      "[15] [4 8]\n",
      "[2] [10  8]\n",
      "[4] [ 4 12]\n"
     ]
    }
   ],
   "source": [
    "observations = replay_buffer.gather_all()[1].numpy().squeeze()\n",
    "print(len(observations))\n",
    "# for i in range(100):\n",
    "#     print(f\"[{i}] {observations[i]}\")\n",
    "\n",
    "print(f\"[{0}] {observations[0]}\")\n",
    "print(f\"[{10}] {observations[10]}\")\n",
    "print(f\"[{15}] {observations[15]}\")\n",
    "print(f\"[{2}] {observations[2]}\")\n",
    "print(f\"[{4}] {observations[4]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_shape = [2]\n",
    "memory = episodic_memory.EpisodicMemory(\n",
    "    observation_shape=observation_shape,\n",
    "    observation_compare_fn=embedding_similarity,\n",
    "    capacity=150)\n",
    "\n",
    "memory.add(observation=observations[0], info=dict())\n",
    "memory.add(observation=observations[10], info=dict())\n",
    "memory.add(observation=observations[15], info=dict())\n",
    "memory.add(observation=observations[2], info=dict())\n",
    "memory.add(observation=observations[4], info=dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.99556546 0.99941687 1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "similarities = memory.similarity(observations[0])\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4711529108389231"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4711529108389231"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.read_csv(f\"avrora_synthetic_saved_states.csv\")\n",
    "perf_states  = {}\n",
    "flags = [7, 12]\n",
    "def synthethic_run(flags):\n",
    "    assert len(flags) == 2, \"Amount of flags is not 2\"\n",
    "    row = new_df[((new_df[\"MaxTenuringThreshold\"] == flags[0]) & (new_df[\"ParallelGCThreads\"] == flags[1]))].values.squeeze()\n",
    "    goal = row[2]\n",
    "    return goal\n",
    "\n",
    "synthethic_run(flags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
